<?xml version="1.0" encoding="UTF-8"?>
<p>Gradient boosting machine as well as GBM‐BRT use a boosting approach where datasets are resampled several times to generate results that form a weighted average of the resampled dataset. This is done by creating a gradient (or step‐by‐step) boosting by minimizing errors among series of decision trees that together form a single predictive model (Natekin &amp; Knoll, 
 <xref rid="ece34919-bib-0070" ref-type="ref">2013</xref>; Olinsky, Kennedy, &amp; Kennedy, 
 <xref rid="ece34919-bib-0075" ref-type="ref">2012</xref>; Wana &amp; Beierkuhnlein, 
 <xref rid="ece34919-bib-0096" ref-type="ref">2010</xref>; Boser,Guyon, &amp; Vapnik, 1992). In our study, we tested two implementations of GBM and GBM‐BRT. They are both based on the same packages: “gbm,” “caret,” “dismo,” and “raster,” with “dismo” and “caret” using the “gbm” package to fit the models. The main differences of the two implementations are the use of different hyper‐parameters. We varied the interaction depth (i.e., tree complexity in GBM‐BRT) which we set to 3 for GBM and was set to 5 for GBM‐BRT, as well as the loss function. While GBM used the “Gaussian” family (Friedman, 2001), GBM‐BRT used the "Bernoulli" (Elith, Leathwick, &amp; Hastie, 
 <xref rid="ece34919-bib-0026" ref-type="ref">2008</xref>). Furthermore, the final selection of number of trees and the learning rate was different. We tuned the models by only varying the number of trees and the number of repeats while other parameters were kept stable using their respective R package default settings (for details see also Supporting information Table 
 <xref rid="ece34919-sup-0001" ref-type="supplementary-material">S1</xref>). Fine‐tuning the number of iterations is done to improve the performance of a model by fitting either many sub‐models or gradient fitting and combining them for final prediction. All models were tuned using the same performance metrics. For the fine‐tuning, we calculated mean change in predictive deviance ±one standard error (Elith et al., 
 <xref rid="ece34919-bib-0027" ref-type="ref">2011</xref>). The optimization of the number of trees improved the performance substantially (Supporting information Figure 
 <xref rid="ece34919-sup-0001" ref-type="supplementary-material">S1</xref>).
</p>
