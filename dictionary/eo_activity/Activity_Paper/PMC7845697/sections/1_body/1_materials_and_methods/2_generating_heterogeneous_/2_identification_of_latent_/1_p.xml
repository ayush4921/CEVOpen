<?xml version="1.0" encoding="UTF-8"?>
<p>We generated latent knowledge features to obtain various types of drug and natural compound information from scientific literature. To this end, we applied a word embedding approach that represents a single word as a real-valued vector in a low-dimensional space (
 <xref ref-type="fig" rid="fig1">Figure 1A</xref>). There are several machine learning-based approaches for word embedding. For example, the word2vec creates embedding vectors of words in a given corpus using context to predict a word (continuous bag-of-words, C-BOW model) or using a word to predict the context (skip-gram model) (
 <xref rid="B66" ref-type="bibr">Mikolov et al., 2013a</xref>; Mikolov et al., 2013b). However, this method is highly dependent on the training corpus, making its application to rare or unusual natural compound and drug names difficult. In particular, the organic chemistry field includes many complex and compound words, such as “alpha-isothiocyanatotoluene.” Thus, the word2vec model cannot be used to appropriately estimate vector representations in the field. To solve this problem, we used fastText: a word representation using the sub-word skip-gram model that learns representations for character 
 <italic>n</italic>-grams based on unlabeled corpora where each word is represented as the sum of the 
 <italic>n</italic>-gram vector representations (
 <xref rid="B7" ref-type="bibr">Bojanowski et al., 2017</xref>; 
 <xref rid="B114" ref-type="bibr">Young and Rusli, 2019</xref>). This model improves the representations of rare words by considering the character level information and internal structure of the words. For example, the natural compound name “alpha-isothiocyanatotoluene” can be estimated by dividing the word into “alpha,” “isothiocyanato,” and “toluene,” which are relatively frequent in the training corpora. The fastText model learns the distributed representations for all character 
 <italic>n</italic>-grams in “alpha-isothiocyanatotoluene” and integrates the sub-word vectors to generate the final embedding vector of “alpha-isothiocyanatotoluene.” In this study, we used the pre-trained fastText model with Wikipedia and Common Crawl (
 <xref rid="B33" ref-type="bibr">Grave et al., 2018</xref>). The model additionally learned from the DrugBank indication and PubMed literature. Before training, we pre-processed the PubMed literature by tokenizing each word and transforming it into lowercase. We then transformed special characters and Greek symbols to alphabetic names (e.g., α to alpha) for generalization.
</p>
