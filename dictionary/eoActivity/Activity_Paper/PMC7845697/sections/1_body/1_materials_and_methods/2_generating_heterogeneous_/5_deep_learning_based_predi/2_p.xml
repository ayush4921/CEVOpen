<?xml version="1.0" encoding="UTF-8"?>
<p>When the input features are complex and heterogeneous, deep learning can improve the performance of the predictor by learning high-level representation from low-level features. The proposed model consists of four sequential layers (
 <xref ref-type="fig" rid="fig3">Figure 3</xref>): 1) input layer, 2) partially connected hidden layers, 3) fully connected hidden layers, and 4) output layer. The models were generated for 15 diseases, respectively, to predict the potential effects list from input features. For each drug or natural compound, we generated latent knowledge, molecular interaction, and chemical property features and used them as the inputs of the model. Hidden layers generalized their outputs by providing a high-level representation that was more abstract than the previous layer by discovering nonlinear relationships between the low- and high-level data. Let 
 <italic>X</italic>
 <sub>
  <italic>l</italic>
 </sub> is the output of the 
 <italic>l</italic>th hidden layer. The forward propagation of the neural network with 
 <italic>l</italic>th hidden layer can be represented as follow.
 <disp-formula id="e2">
  <math id="m3">
   <mrow>
    <msub>
     <mi>X</mi>
     <mi>l</mi>
    </msub>
    <mo>=</mo>
    <mi>f</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <msub>
       <mi>W</mi>
       <mi>l</mi>
      </msub>
      <msub>
       <mi>X</mi>
       <mrow>
        <mi>l</mi>
        <mo>−</mo>
        <mn>1</mn>
       </mrow>
      </msub>
      <mo>+</mo>
      <msub>
       <mi>b</mi>
       <mi>l</mi>
      </msub>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </math>
 </disp-formula>where 
 <italic>W</italic>
 <sub>
  <italic>l</italic>
 </sub> = [w
 <sub>
  <italic>l</italic>1
 </sub>, w
 <sub>
  <italic>l</italic>2, … ,
 </sub> w
 <sub>
  <italic>l</italic>n
 </sub>] is the weight matrix of the edge from 
 <italic>l</italic>-1st layer to 
 <italic>l</italic>th layer, 
 <italic>b</italic>
 <sub>
  <italic>l</italic>
 </sub> is the bias of each hidden units, and 
 <italic>f</italic> (·) is the activation function. In this study, the hidden layers were divided to two parts: the partially connected and fully connected parts. A fully connected neural network is the most commonly used model because it usually does not need a priori information on input data for defining the structure of the model (
 <xref rid="B93" ref-type="bibr">Shanmuganathan, 2016</xref>). This simplifies the model design since every neuron in one layer connecting to every neuron in the next layer. However, it may need large training data, and cannot consider the characteristic of the input feature types. A partially connected neural network can be defined as a network that contains only a subset of all possible connections. It has strengths in reducing complexity and improving generalization without producing significant modeling errors. This study applied a partially connected network to learn the spatially distinguished representation of each feature (
 <xref rid="B12" ref-type="bibr">Chen et al., 2016</xref>; 
 <xref rid="B64" ref-type="bibr">Mason et al., 2018</xref>; 
 <xref rid="B102" ref-type="bibr">Tek, 2018</xref>). When input neurons connect to the next layer of neurons, we set them to connect only neurons of the same input feature type. In the above-mentioned weight matrix (
 <italic>W</italic>
 <sub>
  <italic>l</italic>
 </sub>), zero values are set for the disconnected edges based on feature types. When 
 <italic>n</italic> input features are fully connected to 
 <italic>m</italic> neurons included in the hidden layer, 
 <italic>n</italic>·
 <italic>m</italic> edges are created, but the proposed method creates 
 <inline-formula id="inf2">
  <math id="m4">
   <mrow>
    <munder>
     <mstyle displaystyle="true">
      <mo>∑</mo>
     </mstyle>
     <mi>i</mi>
    </munder>
    <msub>
     <mi>n</mi>
     <mi>i</mi>
    </msub>
    <mo>⋅</mo>
    <msub>
     <mi>m</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </math>
 </inline-formula> edges (where 
 <italic>i</italic> is the number of feature types). In this study, the partially connected model generated (101·68) + (285·160) + (300·200) edges, whereas the fully-connected model generated (101 + 285 + 300)·(68 + 190 + 200) edges. We applied a partially connected structure to the first and second hidden layers. This process reduced the number of edges to be trained by about 37%. Therefore, we can learn the weights of the edges with a relatively small training set taking into account the input feature types. The outputs of each partially connected layers are further concatenated to produce the single layer.
</p>
